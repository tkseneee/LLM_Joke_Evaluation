{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d860e4-0740-4c62-84d7-17345dc06cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "import torch\n",
    "import math\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b57b0e9f-5832-4467-b8d8-3c643b0fd294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke: Why don’t scientists trust atoms? Because they make up everything!\n",
      "Coherence (Perplexity): 419.66\n",
      "Humor Score: 0.98\n",
      "\n",
      "Joke: Why did the scarecrow win an award? Because he was outstanding in his field!\n",
      "Coherence (Perplexity): 64.21\n",
      "Humor Score: 0.98\n",
      "\n",
      "Joke: What did the ocean say to the beach? Nothing, it just waved!\n",
      "Coherence (Perplexity): 76.60\n",
      "Humor Score: 0.99\n",
      "\n",
      "Joke: Why do cows have hooves instead of feet? Because they lactose!\n",
      "Coherence (Perplexity): 79.38\n",
      "Humor Score: 0.99\n",
      "\n",
      "Joke: My Name is Senthil\n",
      "Coherence (Perplexity): 178.57\n",
      "Humor Score: 0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of jokes\n",
    "jokes = [\n",
    "    \"Why don’t scientists trust atoms? Because they make up everything!\",\n",
    "    \"Why did the scarecrow win an award? Because he was outstanding in his field!\",\n",
    "    \"What did the ocean say to the beach? Nothing, it just waved!\",\n",
    "    \"Why do cows have hooves instead of feet? Because they lactose!\",\n",
    "    \"My Name is Senthil\"\n",
    "]\n",
    "\n",
    "# Humor detection model\n",
    "humor_model_name = \"mohameddhiab/humor-no-humor\"\n",
    "humor_tokenizer = AutoTokenizer.from_pretrained(humor_model_name)\n",
    "humor_model = AutoModelForSequenceClassification.from_pretrained(humor_model_name)\n",
    "\n",
    "def get_humor_score(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = humor_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    # Get model predictions\n",
    "    outputs = humor_model(**inputs)\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    # The model is trained with label 1 indicating humor\n",
    "    humor_score = probabilities[0][1].item()\n",
    "    return humor_score\n",
    "\n",
    "# GPT-2 model for perplexity calculation\n",
    "gpt2_model_name = \"gpt2\"\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(gpt2_model_name)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(text):\n",
    "    tokens = gpt2_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        loss = gpt2_model(**tokens, labels=tokens[\"input_ids\"]).loss\n",
    "    return torch.exp(loss).item()\n",
    "\n",
    "# Function to evaluate a joke\n",
    "def evaluate_joke(joke):\n",
    "    perplexity = calculate_perplexity(joke)\n",
    "    humor_score = get_humor_score(joke)\n",
    "\n",
    "    return {\n",
    "        \"joke\": joke,\n",
    "        \"perplexity_score\": perplexity,\n",
    "        \"humor_score\": humor_score\n",
    "    }\n",
    "\n",
    "# Evaluate jokes\n",
    "for joke in jokes:\n",
    "    evaluation = evaluate_joke(joke)\n",
    "    print(f\"Joke: {evaluation['joke']}\\nCoherence (Perplexity): {evaluation['perplexity_score']:.2f}\\nHumor Score: {evaluation['humor_score']:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf20229-349b-4eff-a98e-32c0d517e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity can be thought of as the \"uncertainty\" or \"surprise\" of a model when it encounters new text.\n",
    "# A high perplexity value means the model is uncertain or \"surprised\" by the text it is trying to predict, implying that it is not able to predict the next word in the sequence very well.\n",
    "# A low perplexity indicates that the model is confident in its predictions, meaning the text is more predictable to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "907327f3-d802-4a84-a033-b03674f4e0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model_1...\n",
      "BLEU Score: 1.0000\n",
      "\n",
      "Evaluating Model_2...\n",
      "BLEU Score: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the BLEU metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Reference jokes for evaluation (gold standard)\n",
    "reference_jokes = [\n",
    "    [\"Why don’t scientists trust atoms? Because they make up everything!\"],\n",
    "    [\"Why did the scarecrow win an award? Because he was outstanding in his field!\"],\n",
    "]\n",
    "\n",
    "# Generated jokes by different LLMs\n",
    "llm_outputs = {\n",
    "    \"Model_1\": [\n",
    "        \"Why don’t scientists trust atoms? Because they make up everything!\",\n",
    "        \"Why did the scarecrow win an award? Because he was outstanding in his field!\",\n",
    "    ],\n",
    "    \"Model_2\": [\n",
    "        \"What did the ocean say to the beach? Nothing, it just waved!\",\n",
    "        \"Why do cows have hooves instead of feet? Because they lactose!\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "# Evaluate BLEU for each model\n",
    "for model_name, jokes in llm_outputs.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "\n",
    "    # Compute BLEU score\n",
    "    results = bleu.compute(predictions=jokes, references=reference_jokes)\n",
    "    print(f\"BLEU Score: {results['bleu']:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a9e38-2063-4344-82e5-6aaa708df1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High BLEU Scores (close to 1.0) indicate that the model is generating jokes that are very similar to the reference jokes.\n",
    "# Low BLEU Scores suggest that the jokes generated by the model differ significantly from the reference jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcce6325-fe37-4271-945c-82e7e0a48741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Model_1:\n",
      "Joke 1: {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0}\n",
      "Joke 2: {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0}\n",
      "\n",
      "Results for Model_2:\n",
      "Joke 1: {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
      "Joke 2: {'rouge1': 0.16, 'rouge2': 0.0, 'rougeL': 0.16}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Reference jokes (list of lists, each containing one reference joke)\n",
    "reference_jokes = [\n",
    "    [\"Why don’t scientists trust atoms? Because they make up everything!\"],\n",
    "    [\"Why did the scarecrow win an award? Because he was outstanding in his field!\"],\n",
    "]\n",
    "\n",
    "# Generated jokes by different LLMs (list of jokes per model)\n",
    "llm_outputs = {\n",
    "    \"Model_1\": [\n",
    "        \"Why don’t scientists trust atoms? Because they make up everything!\",\n",
    "        \"Why did the scarecrow win an award? Because he was outstanding in his field!\",\n",
    "    ],\n",
    "    \"Model_2\": [\n",
    "        \"What did the ocean say to the beach? Nothing, it just waved!\",\n",
    "        \"Why do cows have hooves instead of feet? Because they lactose!\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Function to calculate ROUGE scores for jokes by different models\n",
    "def calculate_rouge_scores(reference_jokes, llm_outputs):\n",
    "    results = {}\n",
    "    \n",
    "    for model_name, generated_jokes in llm_outputs.items():\n",
    "        rouge_scores = []\n",
    "        for ref_joke, gen_joke in zip(reference_jokes, generated_jokes):\n",
    "            # Each reference joke is in a list, so extract it\n",
    "            ref_joke = ref_joke[0]\n",
    "            \n",
    "            # Calculate ROUGE score for each pair of reference and generated joke\n",
    "            scores = scorer.score(ref_joke, gen_joke)\n",
    "            rouge_scores.append({\n",
    "                'rouge1': scores['rouge1'].fmeasure,\n",
    "                'rouge2': scores['rouge2'].fmeasure,\n",
    "                'rougeL': scores['rougeL'].fmeasure\n",
    "            })\n",
    "        \n",
    "        results[model_name] = rouge_scores\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Calculate ROUGE scores for all models\n",
    "rouge_scores = calculate_rouge_scores(reference_jokes, llm_outputs)\n",
    "\n",
    "# Print the ROUGE scores for each model\n",
    "for model_name, scores in rouge_scores.items():\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    for i, score in enumerate(scores):\n",
    "        print(f\"Joke {i+1}: {score}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "242d565d-cd0e-4edf-8002-66950b84f0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE-1 measures unigram overlap.\n",
    "# ROUGE-2 measures bigram overlap.\n",
    "# ROUGE-L measures the longest common subsequence (LCS), which captures word order.\n",
    "# This approach can help you evaluate the quality of jokes generated by different models in comparison to reference jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f14a6d-162a-4021-91da-686a4dc566cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
