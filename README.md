# LLM_Joke_Evaluation
Approach to evaluate the jokes generated through the tuned LLM model

# Joke Evaluation Using Tuned LLM Models

This repository contains the evaluation workflow for jokes generated by tuned large language models (LLMs). It is part of a broader project exploring humor generation and refinement using machine learning techniques, including pre-trained, fine-tuned, and PEFT-based models.

## Overview

The evaluation script, implemented in the provided Jupyter Notebook, calculates several metrics to assess the quality of generated jokes. Each metric offers a unique perspective on the effectiveness of the jokes.

## Evaluation Metrics

1. **Humor Score**:

   - A pretrained humor detection model is used to compute a numerical score reflecting how "funny" each joke is.
   - This score is essential for determining the primary objective: humor.

2. **Perplexity**:

   - Calculated using GPT-2, this metric measures the fluency and coherence of the generated text.
   - Lower perplexity generally indicates more natural and fluent language generation.

3. **BLEU Score**:

   - Evaluates the similarity between generated jokes and reference jokes based on n-gram overlap.
   - While commonly used in text generation, it may not fully capture the nuances of humor.

4. **ROUGE Metrics**:

   - Measures the overlap of words or phrases between generated and reference jokes.
   - Like BLEU, it has limitations when applied to creative and context-dependent content like humor.

## Key Insights

- **Humor Score** and **Perplexity** provide direct insights into the quality and fluency of the jokes.
- **BLEU** and **ROUGE**, although widely used, are not ideal for evaluating jokes due to the subjective and creative nature of humor.

## Files

- **Evaluation\_metric\_for\_Joke-LLM.ipynb**:
  The main Jupyter Notebook containing the code to compute all metrics.

## Dependencies

To run the notebook, ensure the following Python libraries are installed:

- `transformers`
- `torch`
- `numpy`
- `evaluate`

You can install the dependencies using:

```bash
pip install transformers torch numpy scikit-learn nltk datasets
```

## Usage

1. Clone the repository:

   ```bash
   git clone [repository_url]
   cd [repository_directory]
   ```

2. Open the notebook in Jupyter:

   ```bash
   jupyter notebook Evaluation_metric_for_Joke-LLM.ipynb
   ```

3. Follow the instructions in the notebook to evaluate jokes and compute the metrics.

## Next Steps

The next phase of this project involves integrating reinforcement learning with human feedback (RLHF) to:

- Improve joke quality further.
- Ensure jokes are non-toxic and unbiased.

Stay tuned for updates on this exciting development!

## License

This project is licensed under the MIT License. See the LICENSE file for details.

## Contact

For questions or suggestions, please reach out to [Your Contact Information].

